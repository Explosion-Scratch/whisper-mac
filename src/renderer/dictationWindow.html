<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Dictation</title>
    <script>
      if (window.__electronLog) {
        Object.assign(console, window.__electronLog);
      }
    </script>
    <script src="https://unpkg.com/@phosphor-icons/web"></script>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      html {
        background: transparent;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display",
          "Helvetica Neue", Arial, sans-serif;
        color-scheme: light dark;
        background: rgba(250, 250, 252, 0.06);
        border-radius: 12px;
        border: 1px solid rgba(255, 255, 255, 0.35);
        box-shadow: 0 12px 48px rgba(0, 0, 0, 0.2),
          0 2px 10px rgba(0, 0, 0, 0.08);
        overflow: hidden;
        user-select: none;
        -webkit-user-select: none;
        transition: opacity 0.25s cubic-bezier(0.4, 0, 0.2, 1),
          transform 0.25s cubic-bezier(0.4, 0, 0.2, 1), box-shadow 0.2s ease,
          border-color 0.2s ease, background 0.2s ease;
        background-clip: padding-box;
        opacity: 0;
        transform: scale(0.95) translateY(10px);
      }

      body.visible {
        opacity: 1;
        transform: scale(1) translateY(0);
      }

      body:hover {
        box-shadow: 0 14px 56px rgba(0, 0, 0, 0.22),
          0 6px 16px rgba(0, 0, 0, 0.1);
      }

      .dictation-container {
        padding: 12px 16px;
        display: flex;
        align-items: center;
        gap: 12px;
        height: 48px;
        min-height: 48px;
        max-height: 48px;
        -webkit-app-region: drag; /* Make the entire container draggable */
        cursor: grab;
        transition: cursor 0.2s ease;
      }

      .dictation-container:active {
        cursor: grabbing;
      }

      /* Make interactive elements non-draggable */
      .status-icn,
      .close-button,
      .text-scroll-container {
        -webkit-app-region: no-drag;
        cursor: default;
      }

      .close-button {
        cursor: pointer;
      }

      .text-scroll-container {
        cursor: text;
      }

      .status-icn {
        width: 24px;
        height: 24px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        flex-shrink: 0;
        transition: all 0.3s ease;
        /* stronger, simple border + shadow for better contrast */
        border: 1px solid rgba(0, 0, 0, 0.08);
        box-shadow: 0 1px 2px rgba(0, 0, 0, 0.14);
        position: relative;
      }

      .loading-circle {
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%);
        --size: 20px;
        width: var(--size);
        height: var(--size);
        border-radius: 50%;
        --width: 1px;
        border: var(--width) solid transparent;
        border-top: var(--width) solid white;
        animation: loadingSpin 0.6s linear infinite;
        z-index: 10;
        opacity: 0;
        transition: opacity 0.3s ease;
      }

      .loading-circle.active {
        opacity: 1;
      }

      @keyframes loadingSpin {
        0% {
          transform: translate(-50%, -50%) rotate(0deg);
        }
        100% {
          transform: translate(-50%, -50%) rotate(360deg);
        }
      }

      .status-icn.recording {
        background: rgba(255, 105, 97, 0.92);
        animation: pulse 1.5s infinite;
      }

      .status-icn.idle {
        background: rgba(52, 199, 89, 0.92);
      }

      .status-icn.transcribing,
      .status-icn.transforming,
      .status-icn.injecting {
        background: rgba(255, 159, 10, 0.92);
      }

      .status-icn.complete {
        background: rgba(52, 199, 89, 0.92);
      }

      .status-icn i {
        font-size: 14px;
        color: white;
      }

      .text-scroll-container {
        flex: 1;
        overflow-x: auto;
        overflow-y: hidden;
        position: relative;
        height: 24px;
        display: flex;
        align-items: center;
        scrollbar-width: none;
        -ms-overflow-style: none;
      }

      .text-scroll-container::-webkit-scrollbar {
        display: none;
      }

      .wave-container {
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transition: opacity 0.25s ease;
        pointer-events: none;
      }

      .wave-container.active {
        opacity: 1;
      }

      .wave-canvas {
        width: 100%;
        height: 24px;
        image-rendering: pixelated;
      }

      .text-content {
        display: flex;
        align-items: baseline;
        gap: 4px;
        white-space: nowrap;
        min-width: 100%;
        line-height: 1.2;
      }

      .text-segment {
        font-size: 13px;
        color: #0b0b0c66; /* increased contrast */
        font-weight: 300;
        display: inline-block;
        line-height: 1.2;
        text-shadow: none; /* remove soft glow to improve legibility */
      }

      .text-segment.transcribed {
        color: #0b0b0c;
      }

      .text-segment.in-progress {
        color: #6b6b70; /* darker grey for better contrast */
      }

      .text-segment.in-progress::after {
        content: "|";
        animation: blink 1.2s ease-in-out infinite;
        margin-left: 1px;
        font-weight: 300;
        vertical-align: baseline;
        line-height: 1.2;
      }

      .text-segment.in-progress.no-caret::after {
        content: none;
      }

      .in-progress-spinner {
        width: 14px;
        height: 14px;
        animation: spin 0.9s linear infinite;
        display: inline-block;
        vertical-align: text-bottom;
        margin-left: 4px;
      }

      .separator {
        color: #6e6e73;
        font-size: 12px;
        margin: 0 4px;
        text-shadow: none;
      }

      @keyframes pulse {
        0% {
          transform: scale(1);
          opacity: 1;
        }
        50% {
          transform: scale(1.1);
          opacity: 0.8;
        }
        100% {
          transform: scale(1);
          opacity: 1;
        }
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      @keyframes fastSpin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .status-icn.transcribing i {
        animation: fastSpin 0.6s linear infinite;
      }

      @keyframes blink {
        0% {
          opacity: 1;
        }
        50% {
          opacity: 0.3;
        }
        100% {
          opacity: 1;
        }
      }

      .close-button {
        width: 20px;
        height: 20px;
        border-radius: 50%;
        background: rgba(255, 59, 48, 0.35);
        border: none;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        flex-shrink: 0;
        box-shadow: inset 0 0 0 0.5px rgba(255, 255, 255, 0.6),
          0 1px 2px rgba(0, 0, 0, 0.08);
      }

      .close-button:hover {
        background: rgba(255, 69, 58, 0.5);
        transform: translateY(-0.5px);
      }

      .close-button i {
        font-size: 10px;
        color: white;
      }

      @media (prefers-color-scheme: dark) {
        body {
          background: rgba(30, 30, 30, 0.5);
          border-color: rgba(255, 255, 255, 0.12);
          box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5),
            0 6px 18px rgba(0, 0, 0, 0.3);
          background-image: linear-gradient(
            to bottom,
            rgba(255, 255, 255, 0.06),
            rgba(255, 255, 255, 0.02)
          );
        }

        body:hover {
          box-shadow: 0 22px 70px rgba(0, 0, 0, 0.55),
            0 8px 22px rgba(0, 0, 0, 0.35);
        }

        .text-segment {
          color: #f2f2f7;
          text-shadow: 0 1px 1px rgba(0, 0, 0, 0.35);
        }

        .text-segment.transcribed {
          color: #f2f2f7;
        }

        .text-segment.in-progress {
          color: #8e8e93;
        }

        .separator {
          color: #8e8e93;
          text-shadow: 0 1px 1px rgba(0, 0, 0, 0.25);
        }

        .loading-circle {
          border-top-color: white;
        }
      }

      @media (prefers-reduced-transparency: reduce) {
        body {
          backdrop-filter: none;
          -webkit-backdrop-filter: none;
          background: #f5f5f7;
          border-color: rgba(0, 0, 0, 0.1);
        }
        @media (prefers-color-scheme: dark) {
          body {
            background: #1c1c1e;
            border-color: rgba(255, 255, 255, 0.12);
          }
        }
      }
    </style>
  </head>
  <body>
    <div id="app">
      <div class="dictation-container">
        <div class="status-icn" :class="currentStatus">
          <div
            class="loading-circle"
            :class="{ active: currentStatus === 'transforming' || currentStatus === 'injecting' }"
          ></div>
          <!-- Microphone icon for idle and recording states -->
          <i
            v-if="currentStatus === 'idle' || currentStatus === 'recording'"
            class="ph-duotone ph-microphone"
          ></i>

          <!-- Spinner icon for transcribing state -->
          <i
            v-else-if="currentStatus === 'transcribing'"
            class="ph ph-spinner"
          ></i>

          <!-- Sparkle icon for transforming state -->
          <i
            v-else-if="currentStatus === 'transforming'"
            class="ph-duotone ph-sparkle"
          ></i>

          <!-- Paper plane icon for injecting state -->
          <i
            v-else-if="currentStatus === 'injecting'"
            class="ph-duotone ph-paper-plane-tilt"
          ></i>

          <!-- Check icon for complete state -->
          <i
            v-else-if="currentStatus === 'complete'"
            class="ph-duotone ph-check-circle"
          ></i>
        </div>

        <div class="text-scroll-container" ref="textScrollContainer">
          <div class="wave-container" :class="{ active: showVisualizer }">
            <canvas id="waveCanvas" class="wave-canvas" height="24"></canvas>
          </div>
          <div class="text-content" ref="textContent">
            <template v-if="!showVisualizer">
              <template v-if="displaySegments.length > 0">
                <span
                  v-for="(segment, index) in displaySegments"
                  :key="segment.id || index"
                  class="text-segment"
                  :class="[getSegmentClass(segment), { 'no-caret': currentStatus === 'transcribing' }]"
                >
                  <!-- Completed Segment -->
                  <template v-if="segment.completed"
                    >{{ segment.text }}</template
                  >
                  <!-- In-Progress Segment -->
                  <template v-else>
                    <span
                      >{{ segment.text || (currentStatus === 'transcribing' ?
                      'Transcribing...' : '') }}</span
                    >
                    <i
                      v-if="currentStatus === 'transcribing'"
                      class="ph ph-spinner in-progress-spinner"
                    ></i>
                  </template>
                </span>
              </template>
              <!-- Fallbacks for when there are no segments -->
              <template v-else>
                <span
                  v-if="currentStatus === 'transcribing'"
                  class="text-segment in-progress"
                  >Transcribing...<i
                    class="ph ph-spinner in-progress-spinner"
                  ></i
                ></span>
                <span
                  v-else-if="currentStatus === 'transforming' || currentStatus === 'injecting'"
                  class="text-segment in-progress"
                  >Processing...</span
                >
                <span v-else class="text-segment" style="opacity: 0"
                  >&nbsp;</span
                >
              </template>
            </template>
          </div>
        </div>

        <button class="close-button" @click="handleClose">
          <i class="ph-duotone ph-x"></i>
        </button>
      </div>
    </div>
    <!-- Audio elements for sound feedback -->
    <audio id="startSound" preload="auto">
      <source src="../assets/start.mp3" type="audio/mpeg" />
    </audio>
    <audio id="endSound" preload="auto">
      <source src="../assets/end.mp3" type="audio/mpeg" />
    </audio>
    <!-- Silero VAD Libraries (local) -->
    <script src="./ort.js"></script>
    <script src="./bundle.min.js"></script>
    <script src="./vue.js"></script>
    <script src="./audio-visualizer.js"></script>
    <script>
      const { createApp, ref, computed, onMounted, onUnmounted, nextTick, watch } = Vue;

      createApp({
        setup() {
          // Reactive state
          const isRecording = ref(false);
          const currentStatus = ref("idle");
          const transcriptionSegments = ref([]);
          const finalText = ref("");
          const currentAudioLevel = ref(0);
          const isRunOnAllPlugin = ref(false);
          const selectedText = ref("");
          const isSpeaking = ref(false);

          const displaySegments = computed(() => {
            if (finalText.value) {
              return [
                { type: "transcribed", text: finalText.value, completed: true },
              ];
            }

            const segments = transcriptionSegments.value;
            const completedSegments = segments.filter(
              (segment) => segment.type === "transcribed" && segment.completed,
            );

            const lastInProgressSegment = segments
              .filter(
                (segment) =>
                  segment.type === "inprogress" ||
                  (!segment.completed && segment.type === "transcribed"),
              )
              .pop();

            const result = [...completedSegments];
            if (lastInProgressSegment) {
              result.push(lastInProgressSegment);
            }

            return result;
          });

          const showVisualizer = computed(() => {
            // Priority 1: If the user is speaking, always show the visualizer.
            if (isSpeaking.value) {
              return true;
            }
            if (currentStatus.value === "idle" && isRecording.value) {
              return true;
            }
            // Priority 2: In these final states, never show the visualizer.
            if (
              ["transforming", "injecting", "complete"].includes(
                currentStatus.value,
              )
            ) {
              return false;
            }

            // Priority 3: If we have text segments to display, hide the visualizer to show them.
            if (displaySegments.value.length > 0) {
              return false;
            }

            // Priority 4: If we are 'transcribing' but have no segments, hide visualizer to show 'Transcribing...'.
            if (
              currentStatus.value === "transcribing" &&
              displaySegments.value.length === 0
            ) {
              return false;
            }

            // Priority 5: If we are in the 'recording' state without any text, show the visualizer.
            if (currentStatus.value === "recording") {
              return true;
            }

            return false; // Default case
          });

          const hasTranscription = computed(() => {
            return transcriptionSegments.value.length > 0 || finalText.value;
          });

          const textContent = ref(null);
          const textScrollContainer = ref(null);
          let visualizer = null;
          let resizeObserver = null;

          // VAD state
          const vadInstance = ref(null);
          const isVadInitialized = ref(false);
          const mediaStream = ref(null);
          let audioContext = null;
          let analyser = null;
          let sourceNode = null;
          let rmsArray = null;
          let allowFinalFlush = false;
          let deviceChangeTimeout = null;

          const scrollToEnd = () => {
            if (textScrollContainer.value) {
              textScrollContainer.value.scrollLeft =
                textScrollContainer.value.scrollWidth;
            }
          };

          // Methods
          const resetTranscription = () => {
            transcriptionSegments.value = [];
            finalText.value = "";
            currentAudioLevel.value = 0;
          };

          // Sound feedback
          const playStartSound = () => {
            try {
              const startSound = document.getElementById("startSound");
              if (startSound) {
                startSound.volume = 0.8;
                startSound.play().catch(() => {});
              }
            } catch (_) {}
          };

          const playEndSound = () => {
            try {
              const endSound = document.getElementById("endSound");
              if (endSound) {
                endSound.volume = 0.8;
                endSound.play().catch(() => {});
              }
            } catch (_) {}
          };

          const getSegmentClass = (segment) => {
            if (segment.type === "transcribed") {
              return segment.completed ? "transcribed" : "in-progress";
            }
            if (segment.type === "inprogress") return "in-progress";
            return "";
          };

          const getSegmentDisplayText = (segment) => {
            return segment.text;
          };

          setInterval(() => {
            if (analyser && rmsArray) {
              try {
                analyser.getFloatTimeDomainData(rmsArray);
                let sum = 0;
                for (let i = 0; i < rmsArray.length; i++) {
                  const v = rmsArray[i];
                  sum += v * v;
                }
                const rms = Math.sqrt(sum / rmsArray.length);
                const scaled = Math.min(1, rms * 8);
                currentAudioLevel.value = scaled;
              } catch (_) {}
            }
          }, 50);

          const handleClose = () => {
            disableVADStream();
            playEndSound();
            window.electronAPI.cancelDictation();
          };

          // VAD Methods
          const initializeVAD = async () => {
            try {
              if (!window.vad) throw new Error("VAD library not loaded");
              
              // Get selected microphone from settings
              const selectedMicrophone = await window.electronAPI.getSelectedMicrophone() || "default";
              
              // Validate device availability if not using default
              if (selectedMicrophone !== "default") {
                const isAvailable = await checkDeviceAvailability(selectedMicrophone);
                if (!isAvailable) {
                  console.log(
                    `Selected device ${selectedMicrophone} is not available, resetting to default`
                  );
                  await window.electronAPI.setSelectedMicrophone("default");
                  // Recursively call with default to avoid code duplication
                  return await initializeVAD();
                }
              }
              
              // Build audio constraints with selected microphone
              const audioConstraints = {
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true,
              };
              
              // Add device ID if not using default
              if (selectedMicrophone !== "default") {
                audioConstraints.deviceId = { exact: selectedMicrophone };
              }
              
              const stream = await navigator.mediaDevices.getUserMedia({
                audio: audioConstraints,
              });
              
              mediaStream.value = stream;
              stream
                .getAudioTracks()
                .forEach((track) => (track.enabled = false));
              const myVAD = await window.vad.MicVAD.new({
                baseAssetPath: "./",
                onnxWASMBasePath: "./",
                model: "v5",
                positiveSpeechThreshold: 0.5,
                negativeSpeechThreshold: 0.35,
                preSpeechPadFrames: 40,
                redemptionFrames: 10,
                frameSamples: 512,
                minSpeechFrames: 3,
                submitUserSpeechOnPause: true,
                stream: stream,
                onSpeechStart: () => {
                  isSpeaking.value = true;
                },
                onSpeechEnd: (audio) => {
                  isSpeaking.value = false;
                  if (
                    (isRecording.value || allowFinalFlush) &&
                    audio.length > 0
                  ) {
                    window.electronAPI.sendAudioSegment(audio);
                    allowFinalFlush = false;
                  }
                },
              });
              vadInstance.value = myVAD;
              isVadInitialized.value = true;
              await startVAD();
            } catch (error) {
              console.error("Failed to initialize VAD:", error);
              // If specific microphone fails, try with default
              try {
                console.log("Falling back to default microphone");
                const stream = await navigator.mediaDevices.getUserMedia({
                  audio: {
                    sampleRate: 16000,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                  },
                });
                
                mediaStream.value = stream;
                stream
                  .getAudioTracks()
                  .forEach((track) => (track.enabled = false));
                const myVAD = await window.vad.MicVAD.new({
                  baseAssetPath: "./",
                  onnxWASMBasePath: "./",
                  model: "v5",
                  positiveSpeechThreshold: 0.5,
                  negativeSpeechThreshold: 0.35,
                  preSpeechPadFrames: 40,
                  redemptionFrames: 10,
                  frameSamples: 512,
                  minSpeechFrames: 3,
                  submitUserSpeechOnPause: true,
                  stream: stream,
                  onSpeechStart: () => {
                    isSpeaking.value = true;
                  },
                  onSpeechEnd: (audio) => {
                    isSpeaking.value = false;
                    if (
                      (isRecording.value || allowFinalFlush) &&
                      audio.length > 0
                    ) {
                      window.electronAPI.sendAudioSegment(audio);
                      allowFinalFlush = false;
                    }
                  },
                });
                vadInstance.value = myVAD;
                isVadInitialized.value = true;
                await startVAD();
              } catch (fallbackError) {
                console.error("Failed to initialize VAD with fallback:", fallbackError);
              }
            }
          };

          const startVAD = async () => {
            if (!isVadInitialized.value) await initializeVAD();
            if (vadInstance.value) await vadInstance.value.start();
          };

          const enableVADStream = async () => {
            if (mediaStream.value) {
              mediaStream.value
                .getAudioTracks()
                .forEach((track) => (track.enabled = true));
              if (!audioContext)
                audioContext = new (window.AudioContext ||
                  window.webkitAudioContext)();
              if (sourceNode) sourceNode.disconnect();
              sourceNode = audioContext.createMediaStreamSource(
                mediaStream.value,
              );
              analyser = audioContext.createAnalyser();
              analyser.fftSize = 512;
              rmsArray = new Float32Array(analyser.fftSize);
              sourceNode.connect(analyser);
            }
          };

          const disableVADStream = async () => {
            if (mediaStream.value) {
              mediaStream.value
                .getAudioTracks()
                .forEach((track) => (track.enabled = false));
            }
            if (sourceNode) sourceNode.disconnect();
            sourceNode = null;
            analyser = null;
            rmsArray = null;
          };

          const stopVAD = async () => {
            if (vadInstance.value) await vadInstance.value.pause();
          };

          const stopMediaStream = () => {
            if (mediaStream.value) {
              mediaStream.value.getTracks().forEach((track) => track.stop());
              mediaStream.value = null;
            }
          };

          const cleanupMediaStream = () => {
            stopMediaStream();
          };

          const cleanupVAD = async () => {
            if (vadInstance.value) {
              try {
                await vadInstance.value.pause();
              } catch (e) {
                console.error("Error pausing VAD:", e);
              }
              vadInstance.value = null;
            }
            cleanupMediaStream();
            isVadInitialized.value = false;
          };

          const checkDeviceAvailability = async (deviceId) => {
            if (deviceId === "default") {
              return true;
            }
            try {
              const devices = await navigator.mediaDevices.enumerateDevices();
              return devices.some(
                (device) =>
                  device.kind === "audioinput" && device.deviceId === deviceId
              );
            } catch (error) {
              console.error("Error checking device availability:", error);
              return false;
            }
          };

          const handleDeviceChange = async () => {
            if (deviceChangeTimeout) {
              clearTimeout(deviceChangeTimeout);
            }
            deviceChangeTimeout = setTimeout(async () => {
              try {
                const selectedMicrophone =
                  (await window.electronAPI.getSelectedMicrophone()) || "default";

                if (selectedMicrophone === "default") {
                  return;
                }

                const isAvailable = await checkDeviceAvailability(
                  selectedMicrophone
                );

                if (!isAvailable) {
                  console.log(
                    `Selected device ${selectedMicrophone} is no longer available, resetting to default`
                  );
                  await window.electronAPI.setSelectedMicrophone("default");
                  const wasInitialized = isVadInitialized.value;
                  const wasRecording = isRecording.value;
                  await cleanupVAD();
                  await initializeVAD();
                  if (wasInitialized && wasRecording) {
                    await startRecording();
                  }
                }
              } catch (error) {
                console.error("Error handling device change:", error);
              }
            }, 500);
          };

          // IPC event handlers
          const initializeDictation = (data) => {
            selectedText.value = data.selectedText || "";
            isRunOnAllPlugin.value = data.isRunOnAll || false;
            resetTranscription();
            isSpeaking.value = false;
            allowFinalFlush = false;
            currentStatus.value = "idle";
          };

          const startRecording = async () => {
            isRecording.value = true;
            allowFinalFlush = false;
            resetTranscription();
            if (!mediaStream.value || !isVadInitialized.value) {
              await initializeVAD();
            }
            await enableVADStream();
            await startVAD();
            nextTick(setupVisualizer);
          };

          const stopRecording = async () => {
            isRecording.value = false;
            allowFinalFlush = true;
            await stopVAD();
            setTimeout(async () => {
              allowFinalFlush = false;
              await disableVADStream();
              teardownVisualizer();
              stopMediaStream();
            }, 320);
          };

          const updateTranscription = (update) => {
            transcriptionSegments.value = update.segments;
          };

          const completeDictation = (text) => {
            finalText.value = text;
            transcriptionSegments.value = [
              { type: "transcribed", text, completed: true },
            ];
          };

          const clearDictation = () => {
            isRecording.value = false;
            resetTranscription();
            isSpeaking.value = false;
            allowFinalFlush = false;
          };

          const flushPendingAudio = async () => {
            console.log("[DictationWindow] Flushing pending audio...");
            if (vadInstance.value) {
              await vadInstance.value.pause();
              await vadInstance.value.start();
            }
            transcriptionSegments.value = [];
            finalText.value = "";
          };

          watch([displaySegments], () => nextTick(scrollToEnd));

          onMounted(async () => {
            console.log("Dictation window mounted, pre-initializing VAD...");
            initializeVAD().catch(err => console.error("VAD pre-init failed:", err));
            
            window.electronAPI.onAnimateIn(async () => {
              document.body.classList.add("visible");
              playStartSound();
            });
            window.electronAPI.onInitializeDictation(initializeDictation);
            window.electronAPI.onStartRecording(startRecording);
            window.electronAPI.onStopRecording(stopRecording);
            window.electronAPI.onTranscriptionUpdate(updateTranscription);
            window.electronAPI.onDictationComplete(completeDictation);
            window.electronAPI.onDictationClear(clearDictation);
            window.electronAPI.onSetStatus(
              (status) => (currentStatus.value = status),
            );
          window.electronAPI.onPlayEndSound(playEndSound);
          window.electronAPI.onWindowHidden(() => {
            stopMediaStream();
          });
          window.electronAPI.onFlushPendingAudio(flushPendingAudio);
          setupVisualizer();
            document.addEventListener("dragstart", (e) => e.preventDefault());
          window.addEventListener("beforeunload", () => {
            playEndSound();
            cleanupMediaStream();
          });
          
          document.addEventListener("visibilitychange", () => {
            if (document.hidden) {
              stopMediaStream();
            }
          });
            
            // Listen for device changes
            navigator.mediaDevices.addEventListener("devicechange", handleDeviceChange);
          });

          onUnmounted(() => {
            if (deviceChangeTimeout) {
              clearTimeout(deviceChangeTimeout);
            }
            navigator.mediaDevices.removeEventListener("devicechange", handleDeviceChange);
            cleanupVAD();
          });

          function setupVisualizer() {
            const container = textScrollContainer.value;
            const canvas = document.getElementById("waveCanvas");
            if (!container || !canvas) return;
            canvas.width = container.offsetWidth;
            if (!visualizer) {
              visualizer = window.createAudioVisualizer(canvas, {
                getLevel: () => currentAudioLevel.value,
                bars: 64,
                smoothing: 0.6,
              });
              visualizer.start();
            }
            if (!resizeObserver) {
              resizeObserver = new ResizeObserver(() => {
                canvas.width = container.offsetWidth;
                if (visualizer) visualizer.resize();
              });
              resizeObserver.observe(container);
            }
          }

          function teardownVisualizer() {
            try {
              if (visualizer && typeof visualizer.stop === "function") {
                visualizer.stop();
              }
            } catch (_) {}
            visualizer = null;
            if (resizeObserver && textScrollContainer.value) {
              try {
                resizeObserver.unobserve(textScrollContainer.value);
              } catch (_) {}
            }
            resizeObserver = null;
            const canvas = document.getElementById("waveCanvas");
            if (canvas && canvas.getContext) {
              const ctx = canvas.getContext("2d");
              if (ctx) {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
              }
            }
          }

          return {
            isRecording,
            currentStatus,
            finalText,
            showVisualizer,
            isRunOnAllPlugin,
            selectedText,
            statusIconClass: computed(() => currentStatus.value),
            hasTranscription,
            displaySegments,
            textContent,
            textScrollContainer,
            resetTranscription,
            getSegmentClass,
            getSegmentDisplayText,
            handleClose,
          };
        },
      }).mount("#app");
    </script>
  </body>
</html>
