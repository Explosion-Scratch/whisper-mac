<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Dictation</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      html {
        background: transparent;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display",
          "Helvetica Neue", Arial, sans-serif;
        color-scheme: light dark;
        background: rgba(250, 250, 252, 0.06);
        border-radius: 12px;
        border: 1px solid rgba(255, 255, 255, 0.35);
        box-shadow: 0 12px 48px rgba(0, 0, 0, 0.2),
          0 2px 10px rgba(0, 0, 0, 0.08);
        overflow: hidden;
        user-select: none;
        -webkit-user-select: none;
        transition: opacity 0.25s cubic-bezier(0.4, 0, 0.2, 1),
          transform 0.25s cubic-bezier(0.4, 0, 0.2, 1), box-shadow 0.2s ease,
          border-color 0.2s ease, background 0.2s ease;
        background-clip: padding-box;
        opacity: 0;
        transform: scale(0.95) translateY(10px);
      }

      body.visible {
        opacity: 1;
        transform: scale(1) translateY(0);
      }

      body:hover {
        box-shadow: 0 14px 56px rgba(0, 0, 0, 0.22),
          0 6px 16px rgba(0, 0, 0, 0.1);
      }

      .dictation-container {
        padding: 12px 16px;
        display: flex;
        align-items: center;
        gap: 12px;
        height: 48px;
        min-height: 48px;
        max-height: 48px;
        -webkit-app-region: drag; /* Make the entire container draggable */
        cursor: grab;
        transition: cursor 0.2s ease;
      }

      .dictation-container:active {
        cursor: grabbing;
      }

      /* Make interactive elements non-draggable */
      .status-icon,
      .close-button,
      .text-scroll-container {
        -webkit-app-region: no-drag;
        cursor: default;
      }

      .close-button {
        cursor: pointer;
      }

      .text-scroll-container {
        cursor: text;
      }

      .status-icon {
        width: 24px;
        height: 24px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        flex-shrink: 0;
        transition: all 0.3s ease;
        /* stronger, simple border + shadow for better contrast */
        border: 1px solid rgba(0, 0, 0, 0.08);
        box-shadow: 0 1px 2px rgba(0, 0, 0, 0.14);
      }

      .status-icon.recording {
        background: rgba(255, 105, 97, 0.92);
        animation: pulse 1.5s infinite;
      }

      .status-icon.idle {
        background: rgba(52, 199, 89, 0.92);
      }

      .status-icon.transforming {
        background: rgba(255, 159, 10, 0.92);
      }

      .status-icon.complete {
        background: rgba(52, 199, 89, 0.92);
      }

      .status-icon svg {
        width: 14px;
        height: 14px;
        fill: white;
      }

      .text-scroll-container {
        flex: 1;
        overflow-x: auto;
        overflow-y: hidden;
        position: relative;
        height: 24px;
        display: flex;
        align-items: center;
        scrollbar-width: none;
        -ms-overflow-style: none;
      }

      .text-scroll-container::-webkit-scrollbar {
        display: none;
      }

      .wave-container {
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transition: opacity 0.25s ease;
        pointer-events: none;
      }

      .wave-container.active {
        opacity: 1;
      }

      .wave-canvas {
        width: 100%;
        height: 24px;
      }

      .text-content {
        display: flex;
        align-items: baseline;
        gap: 4px;
        white-space: nowrap;
        min-width: 100%;
        line-height: 1.2;
      }

      .text-segment {
        font-size: 13px;
        color: #0b0b0c66; /* increased contrast */
        font-weight: 300;
        display: inline-block;
        line-height: 1.2;
        text-shadow: none; /* remove soft glow to improve legibility */
      }

      .text-segment.transcribed {
        color: #0b0b0c;
      }

      .text-segment.in-progress {
        color: #6b6b70; /* darker grey for better contrast */
      }

      .text-segment.in-progress::after {
        content: "|";
        animation: blink 1.2s ease-in-out infinite;
        margin-left: 1px;
        font-weight: 300;
        vertical-align: baseline;
        line-height: 1.2;
      }

      .text-segment.in-progress.no-caret::after {
        content: none;
      }

      .in-progress-spinner {
        width: 14px;
        height: 14px;
        animation: spin 0.9s linear infinite;
        display: inline-block;
        vertical-align: text-bottom;
      }

      .separator {
        color: #6e6e73;
        font-size: 12px;
        margin: 0 4px;
        text-shadow: none;
      }

      @keyframes pulse {
        0% {
          transform: scale(1);
          opacity: 1;
        }
        50% {
          transform: scale(1.1);
          opacity: 0.8;
        }
        100% {
          transform: scale(1);
          opacity: 1;
        }
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      @keyframes fastSpin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .status-icon.transforming svg {
        animation: fastSpin 0.6s linear infinite;
      }

      @keyframes blink {
        0% {
          opacity: 1;
        }
        50% {
          opacity: 0.3;
        }
        100% {
          opacity: 1;
        }
      }

      .close-button {
        width: 20px;
        height: 20px;
        border-radius: 50%;
        background: rgba(255, 59, 48, 0.35);
        border: none;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        flex-shrink: 0;
        box-shadow: inset 0 0 0 0.5px rgba(255, 255, 255, 0.6),
          0 1px 2px rgba(0, 0, 0, 0.08);
      }

      .close-button:hover {
        background: rgba(255, 69, 58, 0.5);
        transform: translateY(-0.5px);
      }

      .close-button svg {
        width: 10px;
        height: 10px;
        fill: white;
      }

      @media (prefers-color-scheme: dark) {
        body {
          background: rgba(30, 30, 30, 0.5);
          border-color: rgba(255, 255, 255, 0.12);
          box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5),
            0 6px 18px rgba(0, 0, 0, 0.3);
          background-image: linear-gradient(
            to bottom,
            rgba(255, 255, 255, 0.06),
            rgba(255, 255, 255, 0.02)
          );
        }

        body:hover {
          box-shadow: 0 22px 70px rgba(0, 0, 0, 0.55),
            0 8px 22px rgba(0, 0, 0, 0.35);
        }

        .text-segment {
          color: #f2f2f7;
          text-shadow: 0 1px 1px rgba(0, 0, 0, 0.35);
        }

        .text-segment.transcribed {
          color: #f2f2f7;
        }

        .text-segment.in-progress {
          color: #8e8e93;
        }

        .separator {
          color: #8e8e93;
          text-shadow: 0 1px 1px rgba(0, 0, 0, 0.25);
        }
      }

      @media (prefers-reduced-transparency: reduce) {
        body {
          backdrop-filter: none;
          -webkit-backdrop-filter: none;
          background: #f5f5f7;
          border-color: rgba(0, 0, 0, 0.1);
        }
        @media (prefers-color-scheme: dark) {
          body {
            background: #1c1c1e;
            border-color: rgba(255, 255, 255, 0.12);
          }
        }
      }
    </style>
  </head>
  <body>
    <div id="app">
      <div class="dictation-container">
        <div class="status-icon" :class="statusIconClass">
          <!-- Microphone icon for idle state using Phosphor -->
          <svg v-if="currentStatus === 'idle'" viewBox="0 0 256 256">
            <path
              fill="currentColor"
              d="M128 174a46.06 46.06 0 0 0 46-46V64a46 46 0 0 0-92 0v64a46.06 46.06 0 0 0 46 46M94 64a34 34 0 0 1 68 0v64a34 34 0 0 1-68 0Zm40 141.75V240a6 6 0 0 1-12 0v-34.25A78.09 78.09 0 0 1 50 128a6 6 0 0 1 12 0a66 66 0 0 0 132 0a6 6 0 0 1 12 0a78.09 78.09 0 0 1-72 77.75"
            />
          </svg>

          <!-- Microphone icon for listening state using Phosphor -->
          <svg v-else-if="currentStatus === 'listening'" viewBox="0 0 256 256">
            <g fill="currentColor">
              <path
                d="M168 64v64a40 40 0 0 1-40 40a40 40 0 0 1-40-40V64a40 40 0 0 1 40-40a40 40 0 0 1 40 40"
                opacity=".2"
              />
              <path
                d="M128 176a48.05 48.05 0 0 0 48-48V64a48 48 0 0 0-96 0v64a48.05 48.05 0 0 0 48 48M96 64a32 32 0 0 1 64 0v64a32 32 0 0 1-64 0Zm40 143.6V240a8 8 0 0 1-16 0v-32.4A80.11 80.11 0 0 1 48 128a8 8 0 0 1 16 0a64 64 0 0 0 128 0a8 8 0 0 1 16 0a80.11 80.11 0 0 1-72 79.6"
              />
            </g>
          </svg>

          <!-- Loading spinner for transforming state -->
          <svg v-else-if="currentStatus === 'transforming'" viewBox="0 0 24 24">
            <g fill="none" fill-rule="evenodd">
              <path
                d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"
              />
              <path
                fill="currentColor"
                d="M12 4a8 8 0 1 0 0 16a8 8 0 0 0 0-16M2 12C2 6.477 6.477 2 12 2s10 4.477 10 10s-4.477 10-10 10S2 17.523 2 12"
                opacity=".1"
              />
              <path
                fill="currentColor"
                d="M12 4a7.96 7.96 0 0 0-5.533 2.222a1 1 0 1 1-1.384-1.444A9.96 9.96 0 0 1 12 2a1 1 0 1 1 0 2"
              />
            </g>
          </svg>

          <!-- Check icon for complete state using Phosphor -->
          <svg v-else-if="currentStatus === 'complete'" viewBox="0 0 256 256">
            <path
              fill="currentColor"
              d="m229.66 77.66l-128 128a8 8 0 0 1-11.32 0l-56-56a8 8 0 0 1 11.32-11.32L96 188.69L218.34 66.34a8 8 0 0 1 11.32 11.32"
            />
          </svg>
        </div>

        <div class="text-scroll-container" ref="textScrollContainer">
          <div class="wave-container" :class="{ active: showVisualizer }">
            <canvas id="waveCanvas" class="wave-canvas" height="24"></canvas>
          </div>
          <div class="text-content" ref="textContent">
            <span
              v-for="(segment, index) in displaySegments"
              :key="segment.id || index"
              class="text-segment"
              :class="[getSegmentClass(segment), segment.type === 'inprogress' ? 'no-caret' : '']"
            >
              <template v-if="segment.type === 'inprogress'">
                <svg
                  class="in-progress-spinner"
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 256 256"
                >
                  <path
                    fill="currentColor"
                    d="M134 32v32a6 6 0 0 1-12 0V32a6 6 0 0 1 12 0m39.25 56.75A6 6 0 0 0 177.5 87l22.62-22.63a6 6 0 0 0-8.48-8.48L169 78.5a6 6 0 0 0 4.24 10.25ZM224 122h-32a6 6 0 0 0 0 12h32a6 6 0 0 0 0-12m-46.5 47a6 6 0 0 0-8.5 8.5l22.63 22.62a6 6 0 0 0 8.48-8.48ZM128 186a6 6 0 0 0-6 6v32a6 6 0 0 0 12 0v-32a6 6 0 0 0-6-6m-49.5-17l-22.62 22.64a6 6 0 1 0 8.48 8.48L87 177.5a6 6 0 1 0-8.5-8.5M70 128a6 6 0 0 0-6-6H32a6 6 0 0 0 0 12h32a6 6 0 0 0 6-6m-5.64-72.12a6 6 0 0 0-8.48 8.48L78.5 87a6 6 0 1 0 8.5-8.5Z"
                  />
                </svg>
              </template>
              <template v-else> {{ getSegmentDisplayText(segment) }} </template>
            </span>
            <span
              v-if="displaySegments.length === 0"
              class="text-segment in-progress"
              style="opacity: 0"
              >&nbsp;</span
            >
          </div>
        </div>

        <button class="close-button" @click="handleClose">
          <!-- X icon using Phosphor -->
          <svg viewBox="0 0 256 256">
            <path
              d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"
            />
          </svg>
        </button>
      </div>
    </div>
    <!-- Audio elements for sound feedback -->
    <audio id="startSound" preload="auto">
      <source src="../assets/start.mp3" type="audio/mpeg" />
    </audio>
    <audio id="endSound" preload="auto">
      <source src="../assets/end.mp3" type="audio/mpeg" />
    </audio>
    <!-- Silero VAD Libraries (local) -->
    <script src="./ort.js"></script>
    <script src="./bundle.min.js"></script>
    <script src="./vue.js"></script>
    <script src="./audio-visualizer.js"></script>
    <script>
      const { createApp, ref, computed, onMounted, nextTick, watch } = Vue;

      createApp({
        setup() {
          // Reactive state
          const isRecording = ref(false);
          const currentStatus = ref("idle");
          const transcriptionSegments = ref([]);
          const finalText = ref("");
          const currentAudioLevel = ref(0);

          // Computed properties
          const statusIconClass = computed(() => ({
            recording: currentStatus.value === "listening",
            idle: currentStatus.value === "idle",
            transforming: currentStatus.value === "transforming",
            complete: currentStatus.value === "complete",
          }));

          // Visualizer should show when no segments exist OR only in-progress segments exist
          const showVisualizer = computed(() => {
            const segs = transcriptionSegments.value || [];
            if (segs.length === 0) return true;
            const hasCompleted = segs.some(
              (s) => s.type === "transcribed" && s.completed
            );
            return !hasCompleted;
          });

          const hasTranscription = computed(() => {
            return transcriptionSegments.value.length > 0 || finalText.value;
          });

          const displaySegments = computed(() => {
            if (finalText.value) {
              return [
                { type: "transcribed", text: finalText.value, completed: true },
              ];
            }

            // Filter segments to only show the last in-progress segment
            const segments = transcriptionSegments.value;
            const completedSegments = segments.filter(
              (segment) => segment.type === "transcribed" && segment.completed
            );

            // Find the last in-progress segment
            const lastInProgressSegment = segments
              .filter(
                (segment) =>
                  segment.type === "inprogress" ||
                  (!segment.completed && segment.type === "transcribed")
              )
              .pop();

            // Combine completed segments with the last in-progress segment
            const result = [...completedSegments];
            if (lastInProgressSegment) {
              result.push(lastInProgressSegment);
            }

            return result;
          });

          const textContent = ref(null);
          const textScrollContainer = ref(null);
          let visualizer = null;
          let resizeObserver = null;

          // VAD state
          const vadInstance = ref(null);
          const isVadInitialized = ref(false);
          const mediaStream = ref(null);

          const scrollToEnd = () => {
            if (textScrollContainer.value) {
              textScrollContainer.value.scrollLeft =
                textScrollContainer.value.scrollWidth;
            }
          };

          // Methods
          const resetTranscription = () => {
            transcriptionSegments.value = [];
            finalText.value = "";
            currentStatus.value = "listening";
            currentAudioLevel.value = 0;
          };

          // Sound feedback
          const playStartSound = () => {
            try {
              const startSound = document.getElementById("startSound");
              if (startSound) {
                startSound.volume = 0.8;
                startSound.play().catch(() => {});
              }
            } catch (_) {}
          };

          const playEndSound = () => {
            try {
              const endSound = document.getElementById("endSound");
              if (endSound) {
                endSound.volume = 0.8;
                endSound.play().catch(() => {});
              }
            } catch (_) {}
          };

          const getSegmentClass = (segment) => {
            if (segment.type === "transcribed") {
              return segment.completed ? "transcribed" : "in-progress";
            }
            if (segment.type === "inprogress") return "in-progress";
            return "";
          };

          const getSegmentDisplayText = (segment) => {
            return segment.text;
          };

          // Drive visualizer level from VAD envelope when available. Fallback to simple heuristic.
          setInterval(() => {
            if (!showVisualizer.value || currentStatus.value !== "listening")
              return;
            if (typeof window.vad?.getCurrentRms === "function") {
              try {
                const lvl = window.vad.getCurrentRms();
                if (typeof lvl === "number")
                  currentAudioLevel.value = Math.max(0, Math.min(1, lvl));
              } catch (_) {}
            }
          }, 50);

          const handleClose = () => {
            // Disable VAD stream for privacy before hiding
            disableVADStream();
            // Play end sound before hiding
            playEndSound();
            // Close button now hides the window instead of closing it
            window.electronAPI.closeDictationWindow();
          };

          // VAD Methods
          const initializeVAD = async () => {
            try {
              console.log("Initializing Silero VAD...");

              if (!window.vad) {
                throw new Error("VAD library not loaded");
              }

              // Get media stream first
              console.log("Getting media stream...");
              const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                  sampleRate: 16000,
                  channelCount: 1,
                  echoCancellation: true,
                  noiseSuppression: true,
                },
              });

              mediaStream.value = stream;
              console.log("Media stream obtained:", stream);

              // Immediately disable the stream for privacy
              stream.getAudioTracks().forEach((track) => {
                track.enabled = false;
              });
              console.log("Media stream disabled for privacy");

              const myVAD = await window.vad.MicVAD.new({
                baseAssetPath: "./",
                onnxWASMBasePath: "./",
                model: "v5",
                positiveSpeechThreshold: 0.5,
                negativeSpeechThreshold: 0.35,
                preSpeechPadFrames: 40,
                redemptionFrames: 10,
                frameSamples: 512,
                minSpeechFrames: 3,
                submitUserSpeechOnPause: true,
                stream: stream, // Pass the stream to MicVAD
                onSpeechStart: () => {
                  console.log("VAD: Speech started");
                  if (isRecording.value) {
                    currentStatus.value = "listening";
                  }
                },
                onSpeechEnd: (audio) => {
                  console.log("VAD: Speech ended, audio length:", audio.length);
                  if (isRecording.value && audio.length > 0) {
                    // Send audio segment to main process for transcription (plugin-agnostic)
                    window.electronAPI.sendAudioSegment(audio);
                    currentStatus.value = "transforming";
                  }
                },
                onVADMisfire: () => {
                  console.log("VAD: Misfire detected");
                },
                // Optional hook for audio level; if available in library, map to visualizer level
                onAudioFrame: (rms) => {
                  // rms expected 0..1; fallback if not present
                  currentAudioLevel.value = Math.max(0, Math.min(1, rms || 0));
                },
              });

              vadInstance.value = myVAD;
              isVadInitialized.value = true;
              console.log("VAD initialized successfully");

              // Start VAD immediately (stream is already disabled)
              await startVAD();
            } catch (error) {
              console.error("Failed to initialize VAD:", error);
              window.electronAPI.logMessage(
                `VAD initialization failed: ${error.message}`
              );
            }
          };

          const startVAD = async () => {
            if (!isVadInitialized.value) {
              await initializeVAD();
            }

            if (vadInstance.value && isVadInitialized.value) {
              try {
                console.log("Starting VAD...");
                await vadInstance.value.start();
                console.log("VAD started successfully");
              } catch (error) {
                console.error("Failed to start VAD:", error);
                window.electronAPI.logMessage(
                  `VAD start failed: ${error.message}`
                );
              }
            }
          };

          const enableVADStream = async () => {
            if (
              mediaStream.value &&
              mediaStream.value.getAudioTracks().length > 0
            ) {
              try {
                console.log("Enabling VAD stream...");
                // Enable the audio track
                mediaStream.value.getAudioTracks().forEach((track) => {
                  track.enabled = true;
                });
                console.log("VAD stream enabled");
              } catch (error) {
                console.error("Failed to enable VAD stream:", error);
              }
            }
          };

          const disableVADStream = async () => {
            if (
              mediaStream.value &&
              mediaStream.value.getAudioTracks().length > 0
            ) {
              try {
                console.log("Disabling VAD stream...");
                // Disable the audio track
                mediaStream.value.getAudioTracks().forEach((track) => {
                  track.enabled = false;
                });
                console.log("VAD stream disabled");
              } catch (error) {
                console.error("Failed to disable VAD stream:", error);
              }
            }
          };

          const stopVAD = async () => {
            if (vadInstance.value) {
              try {
                console.log("Stopping VAD...");
                await vadInstance.value.pause();
                console.log("VAD stopped successfully");
              } catch (error) {
                console.error("Failed to stop VAD:", error);
                window.electronAPI.logMessage(
                  `VAD stop failed: ${error.message}`
                );
              }
            }
          };

          const cleanupMediaStream = () => {
            if (mediaStream.value) {
              try {
                console.log("Cleaning up media stream...");
                mediaStream.value.getTracks().forEach((track) => track.stop());
                mediaStream.value = null;
                console.log("Media stream cleaned up");
              } catch (error) {
                console.error("Failed to cleanup media stream:", error);
              }
            }
          };

          // IPC event handlers
          const initializeDictation = (data) => {
            console.log("Initializing dictation with data:", data);
            resetTranscription();
          };

          const startRecording = async () => {
            console.log("Recording started");
            isRecording.value = true;
            currentStatus.value = "listening";
            resetTranscription();
            await enableVADStream();
            nextTick(() => {
              setupVisualizer();
            });
          };

          const stopRecording = async () => {
            console.log("Recording stopped");
            isRecording.value = false;
            currentStatus.value = "processing";
            await disableVADStream();
            teardownVisualizer();
          };

          const updateTranscription = (update) => {
            console.log("Transcription update:", update);
            transcriptionSegments.value = update.segments;

            // If status is explicitly provided, use it (for transforming state)
            if (update.status) {
              currentStatus.value = update.status;
              return;
            }

            // Determine status based on segments and current state
            const hasInProgressSegments = update.segments.some(
              (segment) => segment.type === "inprogress" || !segment.completed
            );

            // Only change status if we're not currently transforming
            if (currentStatus.value !== "transforming") {
              if (hasInProgressSegments) {
                currentStatus.value = "listening";
              } else if (update.segments.length > 0) {
                // All segments are completed, show idle state
                currentStatus.value = "idle";
              } else {
                // No segments, default to idle
                currentStatus.value = "idle";
              }
            }
            nextTick(() => {
              if (showVisualizer.value) setupVisualizer();
              else teardownVisualizer();
            });
          };

          const completeDictation = (text) => {
            console.log("Dictation complete:", text);
            finalText.value = text;
            currentStatus.value = "complete";
            transcriptionSegments.value = [
              {
                type: "transcribed",
                text: text,
                completed: true,
              },
            ];
          };

          const clearDictation = () => {
            console.log("Clearing dictation window");
            currentStatus.value = "idle";
            isRecording.value = false;
            resetTranscription();
          };

          // Watch for changes and scroll to end
          watch([displaySegments], () => {
            nextTick(() => {
              scrollToEnd();
            });
          });

          // Play end sound when leaving listening state
          watch([currentStatus], (newStatus, oldStatus) => {
            if (
              oldStatus &&
              oldStatus[0] === "listening" &&
              (newStatus[0] === "transforming" ||
                newStatus[0] === "processing" ||
                newStatus[0] === "complete")
            ) {
              // Removed end sound for segment transitions
            }
          });

          // Setup IPC listeners
          onMounted(async () => {
            if (window.electronAPI && window.electronAPI.onAnimateIn) {
              window.electronAPI.onAnimateIn(() => {
                document.body.classList.add("visible");
                // Play start sound when window opens
                playStartSound();
                // Enable VAD stream when window becomes visible
                enableVADStream();
              });
            }
            window.electronAPI.onInitializeDictation(initializeDictation);
            window.electronAPI.onStartRecording(startRecording);
            window.electronAPI.onStopRecording(stopRecording);
            window.electronAPI.onTranscriptionUpdate(updateTranscription);
            window.electronAPI.onDictationComplete(completeDictation);
            window.electronAPI.onDictationClear(clearDictation);
            window.electronAPI.onPlayEndSound(playEndSound);

            // Visualizer bootstrap
            setupVisualizer();

            // Prevent default drag behavior
            document.addEventListener("dragstart", (e) => e.preventDefault());

            // Add beforeunload listener to play end sound when window is about to close
            window.addEventListener("beforeunload", () => {
              playEndSound();
              cleanupMediaStream();
            });

            // Initialize VAD on mount (stream will be disabled for privacy)
            console.log("Initializing VAD on mount...");
            await initializeVAD();
          });

          function setupVisualizer() {
            try {
              const container = textScrollContainer.value;
              const canvas = document.getElementById("waveCanvas");
              if (!container || !canvas) return;

              canvas.width = container.offsetWidth;
              if (!visualizer) {
                visualizer = window.createAudioVisualizer(canvas, {
                  getLevel: () => currentAudioLevel.value,
                  bars: 64,
                  smoothing: 0.6,
                });
              }
              if (showVisualizer.value) visualizer.start();

              if (!resizeObserver) {
                resizeObserver = new ResizeObserver(() => {
                  canvas.width = container.offsetWidth;
                  if (visualizer) visualizer.resize();
                });
                resizeObserver.observe(container);
              }
            } catch (_) {}
          }

          function teardownVisualizer() {
            try {
              if (visualizer) visualizer.stop();
            } catch (_) {}
          }

          return {
            // State
            isRecording,
            currentStatus,
            transcriptionSegments,
            finalText,
            showVisualizer,

            // Computed
            statusIconClass,
            hasTranscription,
            displaySegments,
            textContent,
            textScrollContainer,

            // Methods
            resetTranscription,
            getSegmentClass,
            getSegmentDisplayText,
            handleClose,
          };
        },
      }).mount("#app");
    </script>
  </body>
</html>
